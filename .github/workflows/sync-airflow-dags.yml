name: Sync Airflow DAGs

on:
  push:
    branches: [main]
    paths:
      - 'data_pipeline/airflow/dags/**'  # Only trigger on DAG changes
  workflow_dispatch:

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r data_pipeline/requirements.txt
      - name: Run tests
        run: |
          pytest data_pipeline/airflow/tests
          echo "Running tests..."
  deploy:
    needs: build-and-test
    runs-on: self-hosted
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
      - name: Copy DAGs to Airflow
        run: |
          sudo rm -rf $HOME/airflow/dags/*
          sudo mkdir -p $HOME/airflow/dags
          sudo chown -R $(whoami) $HOME/airflow/dags
          sudo cp -r data_pipeline/airflow/dags/* $HOME/airflow/dags/
      - name: Restart Airflow
        run: |
          sudo docker restart airflow_airflow-webserver_1
          sudo docker restart airflow_airflow-scheduler_1
      - name: Check Airflow heartbeat
        run: |
          RESPONSE=$(curl -s http://${{ secrets.HOST }}:8080/health)
          echo "Airflow Response: $RESPONSE"
          
          SCHEDULER_HEALTH=$(echo "$RESPONSE" | jq -r '.scheduler.status')
          TRIGGERER_HEALTH=$(echo "$RESPONSE" | jq -r '.triggerer.status')
          
          if [[ "$SCHEDULER_HEALTH" == "healthy" && "$TRIGGERER_HEALTH" == "healthy" ]]; then
            echo "✅ Airflow scheduler and triggerer are healthy"
          else
            echo "❌ Airflow health check failed"
            exit 1
          fi
      - name: Success
        run: echo "✅ DAGs synced successfully!"